# LLM Vulnerability Engineer - API Configuration
# Copy this file to .env and fill in your API keys

# ========================================
# AI Platform API Keys
# ========================================

# Google Gemini API Key
# Get yours at: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Anthropic Claude API Key
# Get yours at: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI GPT-4 API Key
# Get yours at: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Meta Llama API Key (if using hosted service)
# Or leave empty for local deployment
LLAMA_API_KEY=your_llama_api_key_here_or_leave_empty

# ========================================
# Optional Configuration
# ========================================

# Project Settings
PROJECT_NAME=llm-vulnerability-engineer
LOG_LEVEL=INFO
RESULTS_DIR=results

# Rate Limiting (requests per minute)
API_RATE_LIMIT=20

# Test Configuration
MAX_RETRIES=3
TIMEOUT_SECONDS=30

# ========================================
# Development Settings
# ========================================

# Set to 'true' for debug mode
DEBUG_MODE=false

# Verbose output
VERBOSE=false

# Save raw responses
SAVE_RAW=true

# ========================================
# Security Notes
# ========================================
# 
# IMPORTANT: Never commit this file with real API keys!
# Add .env to .gitignore (already done)
# 
# To get API keys:
# - Gemini: https://makersuite.google.com/
# - Claude: https://console.anthropic.com/
# - OpenAI: https://platform.openai.com/
# - Llama: Local or through providers like Replicate
#
# For local Llama deployment:
# - Use Ollama, LM Studio, or similar
# - Update test_prompt.py with your local endpoint
