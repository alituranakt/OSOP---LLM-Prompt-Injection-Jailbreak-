# LLM Vulnerability Prompt Engineer & Jailbreak Scanner


Bu proje, **Generative AI** modellerindeki (GPT-4, Gemini, Claude) gÃ¼venlik aÃ§Ä±klarÄ±nÄ± tespit etmek iÃ§in geliÅŸtirilmiÅŸ bir **AI SecOps** aracÄ±dÄ±r.

## ğŸ“‚ Proje YapÄ±sÄ±
- **src/**: Python tarama scriptleri ve otomasyon araÃ§larÄ±.
- **researchs/**: Prompt Injection ve Jailbreak test sonuÃ§larÄ± (Raporlar).
- **specs/**: Proje teknik dokÃ¼mantasyonu.

## ğŸ›  Kurulum ve KullanÄ±m (HocanÄ±n Ä°stediÄŸi AdÄ±mlar)
1. Projeyi klonlayÄ±n.
2. Sanal ortamÄ± oluÅŸturun: `python -m venv venv`
3. BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin: `pip install -r requirements.txt`
4. `.env` dosyasÄ±nÄ± oluÅŸturun ve API anahtarlarÄ±nÄ±zÄ± girin.
